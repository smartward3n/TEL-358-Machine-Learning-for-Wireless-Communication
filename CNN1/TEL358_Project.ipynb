{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%matplotlib inline\n","%load_ext autoreload\n","%load_ext tensorboard\n","%autoreload 2\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"rtcfWoJPrGr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, sys\n","\n","folder = \"/content/drive/MyDrive/Tel_358_Project/CNN1\"\n","\n","sys.path.append(folder)"],"metadata":{"id":"NxB4IV0qrPNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install QKeras"],"metadata":{"id":"9gTeC1dsiRyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","physical_devices = tf.config.list_physical_devices()\n","for d in physical_devices:\n","  print(d)"],"metadata":{"id":"H5aaJ_jyPVd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TPU for Training Only"],"metadata":{"id":"9i_QnCNPPtVn"}},{"cell_type":"code","source":["try:\n","  device_name = os.environ['COLAB_TPU_ADDR']\n","  TPU_ADDRESS = 'grpc://' + device_name\n","  print('Found TPU at: {}'.format(TPU_ADDRESS))\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  # This is the TPU initialization code that has to be at the beginning.\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","  strategy = tf.distribute.TPUStrategy(resolver)\n","except KeyError:\n","  print('TPU not found')\n","  strategy = tf.distribute.get_strategy()"],"metadata":{"id":"IT0QHpC6PgJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EnK3pYthvBZ"},"outputs":[],"source":["import os,random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.collections import LineCollection\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","import pickle, random, sys\n","import keras\n","import keras.backend as K\n","from keras.callbacks import LearningRateScheduler,TensorBoard\n","from keras.optimizers import Adam\n","import csv\n","import mltools,rmldataset2016\n","import rmlmodels.CNN2Model as cnn2\n","import tensorflow_model_optimization as tfmot"]},{"cell_type":"markdown","source":["### Importing The Dataset"],"metadata":{"id":"EkJnC8SfP3w_"}},{"cell_type":"code","source":["\n","K.set_image_data_format('channels_last')\n","print(K.image_data_format())\n","\n","(mods,snrs,lbl),(X_train,Y_train),(X_val,Y_val),(X_test,Y_test),(train_idx,val_idx,test_idx) = \\\n","    rmldataset2016.load_data()\n","\n","in_shp = list(X_train.shape[1:])\n","print(X_train.shape)\n","classes = mods\n","print(classes)"],"metadata":{"id":"uUTDMq6UtOnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up some params\n","nb_epoch = 1     # number of epochs to train on\n","batch_size = 256  # training batch size\n","filepath = 'weights/weights.h5'"],"metadata":{"id":"ytkFCmybTjA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.backend.clear_session()\n","with strategy.scope():\n","  model = cnn2.CNN2Model(None, input_shape=in_shp,classes=len(classes))\n","  model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n","model.summary()"],"metadata":{"id":"-Z21cVyZuKB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(X_train,\n","    Y_train,\n","    batch_size=batch_size,\n","    epochs=nb_epoch,\n","    verbose=2,\n","    validation_data=(X_val,Y_val),\n","    callbacks = [\n","                keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n","                keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,verbose=1,patince=5,min_lr=0.000001),\n","                keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='auto')\n","                #keras.callbacks.TensorBoard(log_dir='./logs/',histogram_freq=1,write_graph=False,write_grads=1,write_images=False,update_freq='epoch')\n","                ]\n","                    )\n","\n"],"metadata":{"id":"dbt6ym2jF8_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model):\n","    # Plot confusion matrix\n","    model.load_weights(filepath)\n","    test_Y_hat = model.predict(X_test, batch_size=batch_size)\n","    confnorm,_,_ = mltools.calculate_confusion_matrix(Y_test,test_Y_hat,classes)\n","    mltools.plot_confusion_matrix(confnorm, labels=['8PSK','AM-DSB','AM-SSB','BPSK','CPFSK','GFSK','4-PAM','16-QAM','64-QAM','QPSK','WBFM'],save_filename='cnn2_total_confusion')\n","\n","    # Plot confusion matrix\n","    acc = {}\n","    acc_mod_snr = np.zeros( (len(classes),len(snrs)) )\n","    i = 0\n","    for snr in snrs:\n","\n","        # extract classes @ SNR\n","        # test_SNRs = map(lambda x: lbl[x][1], test_idx)\n","        test_SNRs = [lbl[x][1] for x in test_idx]\n","\n","        test_X_i = X_test[np.where(np.array(test_SNRs) == snr)]\n","        test_Y_i = Y_test[np.where(np.array(test_SNRs) == snr)]\n","\n","        # estimate classes\n","        test_Y_i_hat = model.predict(test_X_i)\n","        confnorm_i,cor,ncor = mltools.calculate_confusion_matrix(test_Y_i,test_Y_i_hat,classes)\n","        acc[snr] = 1.0 * cor / (cor + ncor)\n","        result = cor / (cor + ncor)\n","        with open('acc111.csv', 'a', newline='') as f0:\n","            write0 = csv.writer(f0)\n","            write0.writerow([result])\n","        mltools.plot_confusion_matrix(confnorm_i, labels=['8PSK','AM-DSB','AM-SSB','BPSK','CPFSK','GFSK','4-PAM','16-QAM','64-QAM','QPSK','WBFM'], title=\"Confusion Matrix\",save_filename=\"Confusion(SNR=%d)(ACC=%2f).png\" % (snr,100.0*acc[snr]))\n","\n","        acc_mod_snr[:,i] = np.round(np.diag(confnorm_i)/np.sum(confnorm_i,axis=1),3)\n","        i = i +1\n","\n","    #plot acc of each mod in one picture\n","    dis_num=11\n","    for g in range(int(np.ceil(acc_mod_snr.shape[0]/dis_num))):\n","        assert (0 <= dis_num <= acc_mod_snr.shape[0])\n","        beg_index = g*dis_num\n","        end_index = np.min([(g+1)*dis_num,acc_mod_snr.shape[0]])\n","\n","        plt.figure(figsize=(12, 10))\n","        plt.xlabel(\"Signal to Noise Ratio\")\n","        plt.ylabel(\"Classification Accuracy\")\n","        plt.title(\"Classification Accuracy for Each Mod\")\n","\n","        for i in range(beg_index,end_index):\n","            plt.plot(snrs, acc_mod_snr[i], label=classes[i])\n","            # 设置数字标签\n","            for x, y in zip(snrs, acc_mod_snr[i]):\n","                plt.text(x, y, y, ha='center', va='bottom', fontsize=8)\n","\n","        plt.legend()\n","        plt.grid()\n","        plt.savefig('acc_with_mod_{}.png'.format(g+1))\n","        plt.close()\n","    #save acc for mod per SNR\n","    fd = open('acc_for_mod_on_cnn2.dat', 'wb')\n","    pickle.dump(('128','cnn2', acc_mod_snr), fd)\n","    fd.close()\n","\n","    # Save results to a pickle file for plotting later\n","    print(acc)\n","    fd = open('cnn2_d0.5.dat','wb')\n","    pickle.dump( (\"CNN2\", 0.5, acc) , fd )\n","\n","    # Plot accuracy curve\n","    plt.plot(snrs, list(map(lambda x: acc[x], snrs)))\n","    plt.xlabel(\"Signal to Noise Ratio\")\n","    plt.ylabel(\"Classification Accuracy\")\n","    plt.title(\"Classification Accuracy on RadioML 2016.10 Alpha\")\n","    plt.tight_layout()\n","    plt.savefig('each_acc.png')\n","    plt.close()\n","#predict(model_q16)"],"metadata":{"id":"3W4_Ixm9J_5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from qkeras.autoqkeras import *\n","from qkeras import *\n","from qkeras.utils import model_quantize\n","from qkeras.qtools import run_qtools\n","from qkeras.qtools import settings as qtools_settings"],"metadata":{"id":"6d3WIPH4H2Na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model= tf.keras.models.load_model('/content/model_float32_complete.h5')"],"metadata":{"id":"2ET72g4FnE9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pprint\n","reference_internal = \"int8\"\n","reference_accumulator = \"int32\"\n","\n","q = run_qtools.QTools(\n","      model,\n","      # energy calculation using a given process\n","      # \"horowitz\" refers to 45nm process published at\n","      # M. Horowitz, \"1.1 Computing's energy problem (and what we can do about\n","      # it), \"2014 IEEE International Solid-State Circuits Conference Digest of\n","      # Technical Papers (ISSCC), San Francisco, CA, 2014, pp. 10-14,\n","      # doi: 10.1109/ISSCC.2014.6757323.\n","      process=\"horowitz\",\n","      # quantizers for model input\n","      source_quantizers=reference_internal,\n","\n","      is_inference=False,\n","      # absolute path (including filename) of the model weights\n","      # in the future, we will attempt to optimize the power model\n","      # by using weight information, although it can be used to further\n","      # optimize QBatchNormalization.\n","      weights_path=None,\n","      # keras_quantizer to quantize weight/bias in un-quantized keras layers\n","      keras_quantizer=reference_internal,\n","      # keras_quantizer to quantize MAC in un-quantized keras layers\n","      keras_accumulator=reference_accumulator,\n","      # whether calculate baseline energy\n","      for_reference=True)\n","\n","# caculate energy of the derived data type map.\n","energy_dict = q.pe(\n","    # whether to store parameters in dram, sram, or fixed\n","    weights_on_memory=\"sram\",\n","    # store activations in dram or sram\n","    activations_on_memory=\"dram\",\n","    # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n","    min_sram_size=8*16*1024*1024,\n","    # whether load data from dram to sram (consider sram as a cache\n","    # for dram. If false, we will assume data will be already in SRAM\n","    rd_wr_on_io=False)\n","\n","# get stats of energy distribution in each layer\n","energy_profile = q.extract_energy_profile(\n","    qtools_settings.cfg.include_energy, energy_dict)\n","# extract sum of energy of each layer according to the rule specified in\n","# qtools_settings.cfg.include_energy\n","total_energy = q.extract_energy_sum(\n","    qtools_settings.cfg.include_energy, energy_dict)\n","\n","pprint.pprint(energy_profile)\n","print()\n","print(\"Total energy: {:.2f} uJ\".format(total_energy / 1000000.0))"],"metadata":{"id":"Xq3Fm9HwJig5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to TFLite with dynamic range quantization (int8)\n","converter_int8 = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_int8_model = converter_int8.convert()\n","\n","# Save int8 model\n","with open('model_int8.tflite', 'wb') as f:\n","    f.write(tflite_int8_model)\n","\n","# Convert to TFLite with float16 quantization\n","converter_float16 = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter_float16.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter_float16.target_spec.supported_types = [tf.float16]\n","tflite_float16_model = converter_float16.convert()\n","\n","# Save float16 model\n","with open('model_float16.tflite', 'wb') as f:\n","    f.write(tflite_float16_model)"],"metadata":{"id":"1nqFLN47AHbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Calculate model sizes in MB\n","original_size_mb = os.path.getsize('model_float32.h5') / (1024 * 1024)\n","quantized_size_int8_mb = os.path.getsize('model_int8.tflite') / (1024 * 1024)\n","quantized_size_float16_mb = os.path.getsize('model_float16.tflite') / (1024 * 1024)\n","\n","print(f\"Original model size (float32): {original_size_mb:.2f} MB\")\n","print(f\"Quantized model size (float16): {quantized_size_float16_mb:.2f} MB\")\n","print(f\"Quantized model size (int8): {quantized_size_int8_mb:.2f} MB\")"],"metadata":{"id":"UlzdHP7lQJHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate accuracy on test set\n","\n","def evaluate_model(interpreter):\n","    input_index = interpreter.get_input_details()[0]['index']\n","    output_index = interpreter.get_output_details()[0]['index']\n","    num_samples = len(X_test)\n","    correct_predictions = 0\n","\n","    for i in range(num_samples):\n","        input_data = np.expand_dims(X_test[i], axis=0)\n","        interpreter.set_tensor(input_index, input_data)\n","        interpreter.invoke()\n","        output_data = interpreter.get_tensor(output_index)\n","        predicted_label = np.argmax(output_data)\n","        if predicted_label == np.argmax(Y_test[i]):\n","            correct_predictions += 1\n","\n","    accuracy = correct_predictions / num_samples\n","    return accuracy\n","\n","# Evaluate float32 model accuracy on test set using model.predict\n","score = model.evaluate(X_test, Y_test, verbose=1, batch_size=4096)\n","print(f\"Test accuracy (float32):\", (score[1] * 100))\n","\n","\n","# Evaluate float16 model\n","accuracy_float16 = evaluate_model(interpreter_float16)\n","print(f\"Test set accuracy (float16): {accuracy_float16:.4f}\")\n","\n","# Evaluate int8 model\n","accuracy_int8 = evaluate_model(interpreter_int8)\n","print(f\"Test set accuracy (int8): {accuracy_int8:.4f}\")"],"metadata":{"id":"bTZaij9dBPpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pruning\n"],"metadata":{"id":"41vqpr4Izksq"}},{"cell_type":"code","source":["pruning_params= {\n","    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100) # Sparsity Levels: 0.25, 0.5, 0.75\n","}\n","\n","tf.keras.backend.clear_session()\n","\n","with tf.device('/device:GPU:0'):\n","\n","  model= tf.keras.models.load_model('/content/model_float32_complete.h5')\n","\n","  model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n","\n","\n","  model_for_pruning.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n","\n","\n","\n","history = model_for_pruning.fit(X_train,\n","    Y_train,\n","    batch_size=batch_size,\n","    epochs=5,\n","    verbose=2,\n","    validation_data=(X_val,Y_val),\n","\n","    callbacks = [\n","\n","                tfmot.sparsity.keras.UpdatePruningStep()\n","\n","                ]\n","                    )"],"metadata":{"id":"m2QUcvBJ4fyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = model_for_pruning.evaluate(X_test, Y_test, verbose=0, batch_size=4096)\n","\n","model_for_pruning_accuracy = score[1]\n","print(score[1] * 100)"],"metadata":{"id":"-Mki_J116Itw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_pruned = tfmot.sparsity.keras.strip_pruning(model_for_pruning)"],"metadata":{"id":"IKMlkq-rJVKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(model_pruned)\n","pruned_tflite_model = converter.convert()\n","\n","#_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n","\n","with open(\"model_pruned.tflite\", 'wb') as f:\n","  f.write(pruned_tflite_model)\n","\n","print('Saved pruned TFLite model to:', \"/content/model_pruned.tflite\")"],"metadata":{"id":"LeDmLlxZ_RUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tempfile\n","\n","def get_gzipped_model_size(file):\n","  # Returns size of gzipped model, in MB.\n","  import os\n","  import zipfile\n","\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(file)\n","\n","  return os.path.getsize(zipped_file) / (1024 * 1024)"],"metadata":{"id":"4XrarcGgL1Sk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Size of gzipped pruned TFlite model: %.2f MB\" % (get_gzipped_model_size(\"/content/model_pruned.tflite\")))"],"metadata":{"id":"F4P4B8tSL68l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["converter = tf.lite.TFLiteConverter.from_keras_model(model_pruned)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","#converter.target_spec.supported_types = [tf.float16] #FP16\n","quantized_and_pruned_tflite_model = converter.convert()\n","\n","#_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n","\n","with open(\"model_pruned_quantized.tflite\", 'wb') as f:\n","  f.write(quantized_and_pruned_tflite_model)\n","\n","#print('Saved quantized and pruned TFLite model to:', model_pruned_quantized)\n","\n","print(\"Size of gzipped pruned and quantized TFlite model: %.2f MB\" % (get_gzipped_model_size(\"/content/model_pruned_quantized.tflite\")))"],"metadata":{"id":"P-hMMwJgN04L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(interpreter):\n","    input_index = interpreter.get_input_details()[0]['index']\n","    output_index = interpreter.get_output_details()[0]['index']\n","    num_samples = len(X_test)\n","    correct_predictions = 0\n","\n","    for i in range(num_samples):\n","        input_data = np.expand_dims(X_test[i], axis=0)\n","        interpreter.set_tensor(input_index, input_data)\n","        interpreter.invoke()\n","        output_data = interpreter.get_tensor(output_index)\n","        predicted_label = np.argmax(output_data)\n","        if predicted_label == np.argmax(Y_test[i]):\n","            correct_predictions += 1\n","\n","    accuracy = correct_predictions / num_samples\n","    return accuracy"],"metadata":{"id":"ID01zjvCPHLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n","interpreter.allocate_tensors()\n","\n","test_accuracy = evaluate_model(interpreter)\n","\n","print('Pruned and quantized TFLite test_accuracy: {:.4f}'.format(test_accuracy * 100))\n","\n","print('Pruned TF test accuracy: {:.4f}'.format(model_for_pruning_accuracy * 100))"],"metadata":{"id":"-0U8ez3hPX4k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###TensorRT (You Need Linux Based Kernel for This Part)"],"metadata":{"id":"ajYCpuCLI20q"}},{"cell_type":"code","source":["!sudo apt-get install tensorrt"],"metadata":{"id":"LR1EKvzmvd9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import os\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.python.compiler.tensorrt import trt_convert as trt"],"metadata":{"id":"UJ7ZPWPCxrBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.python.client import device_lib\n","\n","def check_tensor_core_gpu_present():\n","    local_device_protos = device_lib.list_local_devices()\n","    for line in local_device_protos:\n","        if \"compute capability\" in str(line):\n","            compute_capability = float(line.physical_device_desc.split(\"compute capability: \")[-1])\n","            if compute_capability>=7.0:\n","                return True\n","\n","print(\"Tensor Core GPU Present:\", check_tensor_core_gpu_present())\n","tensor_core_gpu = check_tensor_core_gpu_present()"],"metadata":{"id":"mdm21Gzaxxa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_pruned.save('saved_model')\n","model = tf.keras.models.load_model('saved_model')"],"metadata":{"id":"vxm8kppC1PgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","num_samples = X_test.shape[0]\n","length = X_test.shape[2]\n","num_channels = X_test.shape[1]\n","\n","batched_input = np.zeros((batch_size, num_channels, length), dtype=np.float32)\n","\n","for i in range(batch_size):\n","  img_index = i % num_samples\n","  x = X_test[img_index]\n","  x = np.expand_dims(x, axis=0)\n","  batched_input[i, :] = x\n","\n","batched_input = tf.constant(batched_input)\n","print('batched_input shape: ', batched_input.shape)\n"],"metadata":{"id":"woPPZYrv4Uxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Benchmarking throughput\n","N_warmup_run = 50\n","N_run = 1000\n","elapsed_time = []\n","\n","for i in range(N_warmup_run):\n","  preds = model.predict(batched_input)\n","\n","for i in range(N_run):\n","  start_time = time.time()\n","  preds = model.predict(batched_input)\n","  end_time = time.time()\n","  elapsed_time = np.append(elapsed_time, end_time - start_time)\n","  if i % 50 == 0:\n","    print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n","\n","print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))"],"metadata":{"id":"wc1Mv1vV6EyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.python.saved_model import tag_constants\n","\n","def benchmark_tftrt(input_saved_model):\n","    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n","    infer = saved_model_loaded.signatures['serving_default']\n","\n","    N_warmup_run = 50\n","    N_run = 1000\n","    elapsed_time = []\n","\n","    input_name = list(infer.structured_input_signature[1].keys())[0]\n","\n","    # Reshape batched_input to match the input shape that the model expects\n","    reshaped_input = tf.expand_dims(batched_input, axis=-1)\n","\n","    for i in range(N_warmup_run):\n","      labeling = infer(**{input_name: reshaped_input})\n","\n","    for i in range(N_run):\n","      start_time = time.time()\n","      labeling = infer(**{input_name: reshaped_input})\n","      end_time = time.time()\n","      elapsed_time = np.append(elapsed_time, end_time - start_time)\n","      if i % 50 == 0:\n","        print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n","\n","    print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))\n","    return (N_run * batch_size / elapsed_time.sum())\n"],"metadata":{"id":"uAguv_eFQAg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Converting to TF-TRT FP32...')\n","\n","converter = trt.TrtGraphConverterV2(input_saved_model_dir='saved_model',\n","                                   precision_mode=trt.TrtPrecisionMode.FP32,\n","                                    max_workspace_size_bytes=8000000000)\n","converter.convert()\n","converter.save(output_saved_model_dir='saved_model_TFTRT_FP32')\n","print('Done Converting to TF-TRT FP32')"],"metadata":{"id":"TnPbRJufx4oP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FP_32_rt = 0\n","\n","for i in range(10):\n","  FP_32_rt += benchmark_tftrt('saved_model_TFTRT_FP32')\n","\n","print('Average Throughput: {:.0f} images/s'.format(FP_32_rt / 10))"],"metadata":{"id":"5fpsfXmK7wA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Converting to TF-TRT FP16...')\n","\n","converter = trt.TrtGraphConverterV2(input_saved_model_dir='saved_model',\n","                                   precision_mode=trt.TrtPrecisionMode.FP16,\n","                                    max_workspace_size_bytes=8000000000)\n","converter.convert()\n","converter.save(output_saved_model_dir='saved_model_TFTRT_FP16')\n","print('Done Converting to TF-TRT FP16')"],"metadata":{"id":"BtKLtruS7_f8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FP_16_rt = 0\n","\n","for i in range(10):\n","  FP_16_rt += benchmark_tftrt('saved_model_TFTRT_FP16')\n","\n","print('Average Throughput: {:.0f} images/s'.format(FP_16_rt / 10))"],"metadata":{"id":"k6KG55b18Wwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calibration_input_fn():\n","    yield (tf.expand_dims(batched_input, axis=-1), )\n","\n","print('Converting to TF-TRT INT8...')\n","\n","converter = trt.TrtGraphConverterV2(input_saved_model_dir='saved_model',\n","                                   precision_mode=trt.TrtPrecisionMode.INT8,\n","                                    max_workspace_size_bytes=8000000000)\n","\n","converter.convert(calibration_input_fn=calibration_input_fn)\n","converter.save(output_saved_model_dir='saved_model_TFTRT_INT8')\n","print('Done Converting to TF-TRT INT8')"],"metadata":{"id":"rM2YcvPZ8nsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INT_8_rt = 0\n","\n","for i in range(10):\n","  INT_8_rt += benchmark_tftrt('saved_model_TFTRT_INT8')\n","\n","print('Average Throughput: {:.0f} images/s'.format(INT_8_rt / 10))"],"metadata":{"id":"i2Q614n49AEv"},"execution_count":null,"outputs":[]}]}